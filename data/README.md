# Data

## Required dataset

Here we describe about the required datasets to create MINT. The first dataset, [Recipe1M+](http://im2recipe.csail.mit.edu/), is a large-scale structured dataset containing over 1 million food names, ingredients, and recipes. We will use this dataset to train the language models, so we can create word embeddings for our main dataset. The full dataset(`layer1.json`) is available for access through their [website](http://im2recipe.csail.mit.edu/). 

Then we would need our training dataset, high-quality generic food items with their list of ingredients and full nutrient composition information. This dataset was curated and shared for research purposes by the nutrition data company [Edamam Inc](https://www.edamam.com/). Edamam dataset will be used to train the nutrition prediction model using their menu item name and nutrition information, and to create sentence embeddings using their menu item name and its list of ingredients. The sentence embeddings will be created by utilizing pre-trained MPNet. The dataset can be accessed through their API or, if you need it for research purpose, contact them for the full dataset.

Dataset for inference, which contains ~75M real-world menu items that contains with about 63% of them coupled with menu description, were also shared by the and shared for research purposes by the nutrition data company [Edamam Inc](https://www.edamam.com/). This dataset will be used to make prediction on nutrient density of real-world restaurants (RND) in different food environment and make our prediction on nutritional quality of the food enviroment (FEND).

In this folder, we included samples of the training dataset `edamam_ingredients_sample.csv`, which includes the menu items and their ingredients, and `edamam_nutrition_sample.csv`, which includes the menu items and their nutrition information. Sample of inference dataset `edamam_inference_sample.csv` is also included.

## Language Models

- Model weights for **RecipeFT** can be downloaded [here](https://drive.google.com/drive/folders/16yGJUie7fu2ZdIwoRbHEGyQU4uLj9jlH)
- **RecipeBERT** can be downloaded through our huggingface repository [here](https://huggingface.co/alexdseo/RecipeBERT)

## Embeddings

You can run [`inf_nandes_FTembed.py`](https://github.com/alexdseo/mint-plus/blob/main/data/inf_nandes_FTembed.py) to create word embeddings for the menu items that does not have description with them. Run [`inf_des_MPNETembed.py`](https://github.com/alexdseo/mint-plus/blob/main/data/inf_des_MPNETembed.py) to create word embeddings for the menu items that have description with them. You have to divide the inference dataset based on if the menu item is coupled with the descripton.

For later steps, you might need embeddings for the training dataset and trained MINT, if so please reference [MINT](https://github.com/alexdseo/mint/blob/main) repository where we explain how to make embeddings for the training dataset and train MINT from scratch. This repository focuses on the large-scale real-world application using MINT and MINT+.

## Generating description and meal type pseudo-labels via ChatGPT

63% of the large-scale real-world restaurant menu dataset are coupled with the menu item description, as this is very valuable information that could potentially help the prediction of nutrient density of the menu, we aim to utilize this information. However, our training dataset does not contain menu item description, since it is not a menu item from a restaurant but a generic food items. Therefore, we take advantage of state-of-the-art generatvie model ChatGPT to create a psuedo-description for our training dataset. You can run [`description_GPT3.5gen_edamam.py`](https://github.com/alexdseo/mint-plus/blob/main/data/description_GPT3.5gen_edamam.py) to generate the description, and you would need an api-key to do so. 

Additionally, when we are predicting the Restaurant Nutrient Desnity (RND) by taking the median of predicted nutrient density of each restaurant menu items, we want to consider the menu items by their meal type because, for example, appetizers and desserts have very different nutrient composition distribution. Thefore, when we are assessing the nutrient density of the food environments we want to check if considering them separately affects the Food Environment Nutirent Density (FEND) scores. For the analysis, we are going to analyze 5 different FEND scores for each food environment, `ALL_FEND` that considers the whole menu items within the restaurant, `APP_FEND` that considers only the appetizers in the menu, `MAIN_FEND` that considers only the main dish in the menu, `DSRT_FEND` that considers only the desserts in the menu, and `DRNK_FEND` that considers only the drinks in the menu. You can run [`AMDD_GPT3.5gen_edamam.py`](https://github.com/alexdseo/mint-plus/blob/main/data/description_GPT3.5gen_edamam.py) to generate the Appetizer, Main dish, Dessert, and Drink labels (AMDD) for the training dataset. You would also need an api-key to do so. Running this file, it is inevitable that ChatGPT hallucinates sometimes and give unexpected answer because there are sauces and condiments included in the training dataset. Especially, appetizers and drink labels were affected by this hallucianation, for example, sauces were labeled as appetizer. For this cases, we sorted out the appetizers, drinks, and other hallucinated answers, and run those instances with ChatGPT again using [`sauce_GPT3.5cls_edamam.py`](https://github.com/alexdseo/mint-plus/blob/main/data/description_GPT3.5gen_edamam.py), where we ask "Is this food item either sauce, dressing, seasoning, or a spice? Answer yes or no.". We only take that are not a sauce, dressing, seasoning, or a spice and have initial AMDD labels from the first run. 

## Inference on large-scale real-world restaurant menu items

Now, we have training dataset that has generated description and AMDD lables. We will now make an inference on the large-scale real-world restaurant menu items. In this step, we will perform a classification task on the inference dataset using a model trained with psuedo-AMDD labels from the training dataset. For the dataset that has descriptions, we tested with embeddings from both pre-trained MPNET and RecipeFT, however there were minimal difference in classification task performance. Therefore, we used RecipeFT embeddings just using the name of the menu items for all dataset. You can run [`AMDD_cls_inf.py`](https://github.com/alexdseo/mint-plus/blob/main/data/AMDD_cls_inf.py) to get the predicted AMDD labels for the inference dataset.

After this task, you will have 2 different embeddings, menu items with description and without, and the pseudo labels for the meal type (AMDD) for the inference dataset. Additionally, the training datset have generated description of the food item and the pseudo labels for the meal type (AMDD) as well. Now we have to predict the nutrient density for the inference dataset using the training datset to perform an analysis evaluating food environments in the United States.
